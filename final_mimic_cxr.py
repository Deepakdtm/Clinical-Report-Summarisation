# -*- coding: utf-8 -*-
"""FINAL MIMIC-CXR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ALT5G_Vvtji3EO0rlyOOvf85MZb7c0--
"""

# ✅ Compatible pins for Google Colab (Aug 2025). Keeps Transformers v4 to avoid API changes.
# Install fsspec first to avoid dependency conflicts
!pip -q install "fsspec==2025.3.0"
!pip -q install -U   "transformers>=4.44,<5"   accelerate   "datasets>=2.19,<3"   "evaluate>=0.4,<0.5"   "rouge-score>=0.1.2,<0.2"   "bert-score>=0.3.13,<0.4"   "pandas==2.2.2"   "scikit-learn<1.7"   "pyarrow>=14,<20"

import transformers, torch, pandas as pd, importlib
print("Transformers:", transformers.__version__)
print("CUDA available:", torch.cuda.is_available())
print("pandas:", pd.__version__)
print("scikit-learn:", importlib.import_module("sklearn").__version__)
print("pyarrow:", importlib.import_module("pyarrow").__version__)

import os, sys, math, re, json, warnings
import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer
import evaluate # Import evaluate
import numpy as np

print("Python:", sys.version)
if not torch.cuda.is_available():
    raise SystemExit("❌ No GPU detected. In Colab, go to Runtime → Change runtime type → Hardware accelerator → GPU, then rerun.")

gpu_name = torch.cuda.get_device_name(0)
vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
print(f"✅ Detected GPU: {gpu_name} | VRAM ≈ {vram_gb:.1f} GB")

# Recommendation note (A100 > L4 > T4); execution continues regardless.
rec = "A100 (best), then L4, then T4."
print("Recommendation:", rec)

# Silence the known T5 GenerationMixin warning (informational only)
warnings.filterwarnings("ignore", message="T5ForConditionalGeneration has generative capabilities")

USE_DRIVE = True  # set False to keep everything in /content
DATA_DIR  = "/content/drive/MyDrive/radiology_summarisation"
OUTPUT_DIR = "/content/drive/MyDrive/radiology_summarisation/models/flan_t5_e2e"

if USE_DRIVE:
    from google.colab import drive
    drive.mount("/content/drive")
else:
    DATA_DIR = "/content"
    OUTPUT_DIR = "/content/models/flan_t5_e2e"

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
print("DATA_DIR:", DATA_DIR)
print("OUTPUT_DIR:", OUTPUT_DIR)

import os
import pandas as pd
from google.colab import files

# Set the path for the uploaded file
REPORTS_CSV_PATH = "mimic_cxr_clean.csv"  # This will be the filename after upload

# Check if the file exists, and prompt for upload if not
if not os.path.exists(REPORTS_CSV_PATH):
    print("Upload your mimic_cxr_clean.csv ...")
    uploaded = files.upload()  # This will prompt for a file upload
    for k in uploaded.keys():
        if k.lower().endswith(".csv"):
            REPORTS_CSV_PATH = k
            break

print("Using:", REPORTS_CSV_PATH)

# Try different encodings to read the file
encodings_to_try = ["utf-8", "utf-8-sig", "cp1252", "latin1"]
last_err = None
df = None

for enc in encodings_to_try:
    try:
        df = pd.read_csv(REPORTS_CSV_PATH, encoding=enc)
        print(f"Loaded CSV with encoding: {enc}")
        break
    except Exception as e:
        last_err = e

# If the DataFrame is not loaded, raise an error
if df is None:
    raise RuntimeError(f"Failed to load CSV at {REPORTS_CSV_PATH}. Last error: {last_err}")

# Display the columns and the first few rows
print("Columns:", df.columns.tolist())
print(df.head())

encodings_to_try = ["utf-8", "utf-8-sig", "cp1252", "latin1"]
df, last_err = None, None
for enc in encodings_to_try:
    try:
        df = pd.read_csv(REPORTS_CSV_PATH, encoding=enc)
        print("Loaded CSV with encoding:", enc)
        break
    except Exception as e:
        last_err = e
if df is None:
    raise RuntimeError(f"Failed to read CSV at {REPORTS_CSV_PATH}. Last error: {last_err}")

print("Columns:", list(df.columns))
display(df.head(3))

required = {"input_text", "target_summary"}
if not required.issubset(df.columns):
    raise ValueError(f"Expected columns {required} not found. Found: {list(df.columns)}")

out = df[["input_text", "target_summary"]].rename(columns={"input_text":"findings", "target_summary":"impression"})
out = out.dropna()
out["findings"] = out["findings"].astype(str).str.strip()
out["impression"] = out["impression"].astype(str).str.strip()
out = out[(out["findings"]!="") & (out["impression"]!="")].reset_index(drop=True)

print("Resolved dataset:", out.shape)
display(out.head(3))

# Save + split
resolved_path = os.path.join(DATA_DIR, "all_resolved.csv")
out.to_csv(resolved_path, index=False)

train_df, temp_df = train_test_split(out, test_size=0.2, random_state=42)
val_df,   test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

for name, d in [("train", train_df), ("val", val_df), ("test", test_df)]:
    d.to_csv(os.path.join(DATA_DIR, f"{name}.csv"), index=False)

print("Saved splits to:", DATA_DIR)

from google.colab import drive
drive.mount('/content/drive')

MODEL_NAME = "google/flan-t5-base"
INPUT_MAX_LEN, TARGET_MAX_LEN = 512, 128

dataset = load_dataset(
    "csv",
    data_files={
        "train": os.path.join(DATA_DIR, "train.csv"),
        "validation": os.path.join(DATA_DIR, "val.csv"),
        "test": os.path.join(DATA_DIR, "test.csv"),
    },
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def preprocess(batch):
    model_inputs = tokenizer(batch["findings"], max_length=INPUT_MAX_LEN, truncation=True)
    # v4-compatible target tokenization
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(batch["impression"], max_length=TARGET_MAX_LEN, truncation=True)
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenised = dataset.map(preprocess, batched=True, remove_columns=["findings","impression"])
tokenised

bf16_ok = hasattr(torch.cuda, "is_bf16_supported") and torch.cuda.is_bf16_supported()

if vram_gb >= 22:   train_bs = eval_bs = 8
elif vram_gb >= 15: train_bs = eval_bs = 4
elif vram_gb >= 10: train_bs = eval_bs = 2
else:               train_bs = eval_bs = 1

target_effective_bs = 16
grad_accum = max(1, math.ceil(target_effective_bs / train_bs))
use_bf16 = bool(bf16_ok)
use_fp16 = (not use_bf16)

# Enable gradient checkpointing for ≤15 GB VRAM
use_grad_ckpt = vram_gb <= 15

print({
    "gpu": gpu_name, "vram_gb": round(vram_gb,1),
    "train_bs": train_bs, "eval_bs": eval_bs,
    "grad_accum": grad_accum, "bf16": use_bf16, "fp16": use_fp16,
    "grad_checkpointing": use_grad_ckpt
})

model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)
if use_grad_ckpt:
    model.gradient_checkpointing_enable()
    print("Enabled gradient checkpointing.")

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    # evaluation_strategy="steps", # Added back evaluation_strategy # Removed evaluation_strategy
    save_strategy="epoch",
    logging_strategy="steps",
    logging_steps=100,
    learning_rate=5e-5,
    per_device_train_batch_size=train_bs,
    per_device_eval_batch_size=eval_bs,
    gradient_accumulation_steps=grad_accum,
    num_train_epochs=3,
    weight_decay=0.01,
    predict_with_generate=True,
    bf16=use_bf16,
    fp16=use_fp16,
    report_to="none",
)

trainer = Seq2SeqTrainer(
    model=model,
    args=args,
    train_dataset=tokenised["train"],
    eval_dataset=tokenised["validation"],
    tokenizer=tokenizer,
    data_collator=data_collator,
)

train_result = trainer.train()
trainer.save_model(OUTPUT_DIR)
print("✅ Training complete. Model saved to:", OUTPUT_DIR)

rouge = evaluate.load("rouge")
bertscore = evaluate.load("bertscore")

def generate_texts(max_samples=256):
    idx = list(range(len(dataset["test"])))[:max_samples]
    preds, refs = [], []
    for i in idx:
        refs.append(dataset["test"][i]["impression"])
        inputs = tokenizer(dataset["test"][i]["findings"], return_tensors="pt", truncation=True, max_length=INPUT_MAX_LEN).to(model.device)
        with torch.no_grad():
            output = model.generate(**inputs, max_length=TARGET_MAX_LEN, num_beams=4)
        preds.append(tokenizer.decode(output[0], skip_special_tokens=True))
    return preds, refs

preds, refs = generate_texts()
rouge_scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)
bert_scores = bertscore.compute(predictions=preds, references=refs, lang="en")

print("ROUGE:", {k: round(v,4) for k,v in rouge_scores.items()})
print("BERTScore F1 (mean):", float(np.mean(bert_scores["f1"])))

# Save metrics
with open(os.path.join(OUTPUT_DIR, "metrics.json"), "w") as f:
    json.dump({"rouge": rouge_scores, "bertscore_f1_mean": float(np.mean(bert_scores["f1"]))}, f, indent=2)
print("Saved metrics to:", os.path.join(OUTPUT_DIR, "metrics.json"))

N = min(10, len(dataset["test"]))
import pandas as pd
samples = pd.DataFrame({
    "findings": [dataset["test"][i]["findings"] for i in range(N)],
    "reference_impression": [dataset["test"][i]["impression"] for i in range(N)],
    "model_summary": preds[:N],
})
samples

pred_path = os.path.join(OUTPUT_DIR, "test_predictions.csv")
pd.DataFrame({"prediction": preds, "reference": refs}).to_csv(pred_path, index=False)
print("Saved predictions to:", pred_path)